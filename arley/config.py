import json
import os
import sys
from enum import StrEnum, auto
from pathlib import Path

import pytz
from typing import Type, Tuple, Optional, Literal, List, Any, Dict, ClassVar

from cachetools import cached, TTLCache
from pydantic import BaseModel, Field, HttpUrl, RootModel, PostgresDsn

_CONFIGDIRPATH: Path = Path(__file__).parent.resolve()
_CONFIGDIRPATH = Path(os.getenv("ARLEY_CONFIG_DIR_PATH")) if os.getenv("ARLEY_CONFIG_DIR_PATH") else _CONFIGDIRPATH

_CONFIGPATH: Path = Path(_CONFIGDIRPATH, "config.yaml")
_CONFIGPATH: Path = Path(os.getenv("ARLEY_CONFIG_PATH")) if os.getenv("ARLEY_CONFIG_PATH") else _CONFIGPATH

_CONFIGLOCALPATH: Path = Path(_CONFIGDIRPATH, "config.local.yaml")
_CONFIGLOCALPATH = Path(os.getenv("ARLEY_CONFIG_LOCAL_PATH")) if os.getenv("ARLEY_CONFIG_LOCAL_PATH") else _CONFIGLOCALPATH


from pydantic_settings import (
    BaseSettings,
    SettingsConfigDict,
    PydanticBaseSettingsSource,
    EnvSettingsSource,
    YamlConfigSettingsSource,
    InitSettingsSource,
    DotEnvSettingsSource,
)

from loguru import logger

# https://buildmedia.readthedocs.org/media/pdf/loguru/latest/loguru.pdf
os.environ["LOGURU_LEVEL"] = os.getenv("LOGURU_LEVEL", "DEBUG")  # standard is DEBUG
logger.remove()  # remove default-handler
logger_fmt: str = "<g>{time:HH:mm:ssZZ}</> | <lvl>{level}</> | <c>{module}::{extra[classname]}:{function}:{line}</> - {message}"
#
logger.add(
    sys.stderr, level=os.getenv("LOGURU_LEVEL"), format=logger_fmt
)  # TRACE | DEBUG | INFO | WARN | ERROR |  FATAL
logger.configure(extra={"classname": "None"})


logger.info(f"EFFECTIVE CONFIGPATH: {_CONFIGPATH}")
logger.info(f"EFFECTIVE CONFIGLOCALPATH: {_CONFIGLOCALPATH}")


# https://docs.pydantic.dev/latest/concepts/pydantic_settings/



class TemplateType(StrEnum):
    plain: str = auto()
    xml: str = auto()
    plain_chat: str = auto()
    xml_chat: str = auto()


class OllamaPrimingMessage(BaseModel):
    role: Literal["system", "assistant", "user"]
    lang: Literal["en", "de"]
    content: str

class Folders(BaseModel):
    old: str = Field(default="WORKED")
    cur: str = Field(default="WORKING")
    rej: str = Field(default="REJECTED")
    err: str = Field(default="FAILED")


class Redis(BaseModel):
    host: str = Field(default="127.0.0.1")
    host_in_cluster: Optional[str] = Field(default=None)
    port: int = Field(default=6379)

class Chromadb(BaseModel):
    host: str = Field(default=os.getenv("CHROMADB_HOST", "127.0.0.1"))
    host_in_cluster: Optional[str] = Field(default=os.getenv("CHROMADB_HOST_CLUSTER", "chromadb.chromadb.svc.cluster.local"))
    port: int = Field(default=int(os.getenv("CHROMADB_PORT", "8000")))
    http_auth_user: Optional[str] = Field(default=None)
    http_auth_pass: Optional[str] = Field(default=None)
    default_collectionname: Optional[str] = Field(default=os.getenv("CHROMADB_DEFAULT_COLLECTIONNAME", "arley"))
    ollama_embed_model: Optional[str] = Field(default=os.getenv("CHROMADB_OLLAMA_EMBED_MODEL", "nomic-embed-text:latest"))

class Ollama(BaseModel):
    host: str = Field(default=os.getenv("OLLAMA_BASE_HOST", "127.0.0.1"))
    port: int = Field(default=int(os.getenv("OLLAMA_PORT", "11434")))
    host_in_cluster: Optional[str] = Field(default=os.getenv("OLLAMA_BASE_HOST_CLUSTER", "ollama.ollama.svc.cluster.local"))
    ollama_model: str = Field(default="llama3.1:latest")
    ollama_embed_model: str = Field(default="nomic-embed-text:latest")
    ollama_function_calling_model: str = Field(default="llama3.1:latest")
    ollama_guess_language_model: str = Field(default="llama3.1:latest")
    ollama_priming_messages: list[OllamaPrimingMessage] = Field(default_factory=list)

class Postgresql(BaseModel):
    host: str = Field(default="127.0.0.1")
    host_in_cluster: Optional[str] = Field(default="postgresql.postgresql.svc.cluster.local")
    username: str = Field(default="postgres")
    password: str = Field(default="empty")
    dbname: str = Field(default="postgres")
    port: int = Field(default=5432)
    # url: Optional[PostgresDsn] = Field(default=None)
    url: Optional[str] = Field(default=None)


class EmailSettings(BaseModel):
    privileged_sender: Optional[str] = Field(default=None)
    mailaddress: str
    mailuser: str
    mailpassword: str
    imapserver: str
    imapport: int
    smtpserver: str
    smtpport: int
    folders: Folders
    alloweddomains: list[str]

# TODO: HT20240912 -> make only_contracts-stuff obsolete by introducing the possibility to define lists of what to include
class ArleyAug(BaseModel):
    template_version: str = Field(default="v2")
    unified: bool = Field(default=False)
    per_item: bool = Field(default=True)
    num_docs: int = Field(default=5)
    template_type: TemplateType = Field(default=TemplateType["xml_chat"])
    only_contracts: bool = Field(default=False)
    lang_filter: bool = Field(default=True)
    first_request_include_aug: bool = Field(default=True)
    first_request_n_aug_results: int = Field(default=1)
    first_request_unified: bool = Field(default=True)
    first_request_per_item: bool = Field(default=False)
    first_request_template_type: TemplateType = Field(default=TemplateType["xml_chat"])
    first_request_aug_only_contracts: bool = Field(default=True)
    first_request_aug_lang_filter: bool = Field(default=True)

class Settings(BaseSettings):
    model_config: ClassVar[SettingsConfigDict] = SettingsConfigDict(
        populate_by_name=True,
        # env_prefix="TAS_",
        case_sensitive=False,
        yaml_file_encoding="utf-8",
        extra="ignore",  # ignore | forbid | allow
        protected_namespaces=(),
        env_nested_delimiter="__",
        # alias_generator=AliasGenerator(
        #     validation_alias=to_camel,
        #     serialization_alias=to_pascal,
        # )
        yaml_file=[_CONFIGPATH, _CONFIGLOCALPATH]
    )

    arley_aug: ArleyAug
    emailsettings: EmailSettings
    redis: Redis
    postgresql: Postgresql
    ollama: Ollama
    chromadb: Chromadb
    timezone: str = Field(default="Europe/Berlin")

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: InitSettingsSource,
        env_settings: EnvSettingsSource,
        dotenv_settings: DotEnvSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return init_settings, env_settings, YamlConfigSettingsSource(settings_cls)



def str2bool(v: str|bool) -> bool:
    if not v:
        return False

    if isinstance(v, bool):
        return v

    return v.lower() in ("yes", "true", "t", "1")


@cached(cache=TTLCache(maxsize=1, ttl=60))
def is_in_cluster() -> bool:
    sa: Path = Path("/var/run/secrets/kubernetes.io/serviceaccount")
    if sa.exists() and sa.is_dir():
        return os.getenv("KUBERNETES_SERVICE_HOST") is not None
    return False



def log_settings():
    for k, v in os.environ.items():
        if k.startswith("PSQL_"):
            logger.info(f"ENV::{k}: {v}")
    logger.info(json.dumps(settings.model_dump(by_alias=True), indent=4, sort_keys=False, default=str))



def get_ollama_options(
        model: str,
        top_k: int|None = 40,
        top_p: float|None = 0.9,
        temperature: float|None = 0.8,
        seed: int|None = 0,
        num_predict: int|None = -1,
        repeat_penalty: float|None = 1.1) -> dict:

    num_ctx: int = get_num_ctx_by_model_name(model_name=model)

    # num_predict	Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)	int	num_predict 42
    # top_k	Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)	int	top_k 40
    # top_p	Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)

    # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values

    options: dict = {}

    if top_k and top_k != 40:  # default: 40
        options["top_k"] = top_k
    if top_p and top_p != 0.9:  # default: 0.9
        options["top_p"] = top_p
    if temperature != 0.8:  # default 0.8
        options["temperature"] = temperature
    if seed and seed != 0:
        options["seed"] = seed
    if repeat_penalty and repeat_penalty != 1.1:
        options["repeat_penalty"] = repeat_penalty

    options["num_gpu"] = -1
    options["num_ctx"] = num_ctx
    options["num_predict"] = num_predict

    # options: dict = {
    #     "num_gpu": -1,  # -1: auto  | 0: none | int: num layers to offload to gpu
    #     "num_ctx": num_ctx,  # 4096 # default: 2048 https://github.com/ollama/ollama/blob/main/docs/faq.md
    #     "num_predict": num_predict,
    # "top_k": top_k,
    # "top_p": top_p,
    # "temperature": temperature,
    # "seed": seed,
    # "repeat_penalty": repeat_penalty,
    # "penalize_newline": penalize_newline,  # present in api-doc, but not in modelfile...
    # "mirostat": 1,
    # "repeat_last_n": 512,
    # "num_keep": 64,  # number of tokens to keep from initial prompt
    # "repeat_penalty": 1.5,
    # "seed": 123,
    # "temperature": 0

    # https://github.com/ollama/ollama/blob/main/docs/api.md
    # "num_keep": 5,
    #     "seed": 42,
    #     "num_predict": 100,
    #     "top_k": 20,
    #     "top_p": 0.9,
    #     "tfs_z": 0.5,
    #     "typical_p": 0.7,
    #     "repeat_last_n": 33,
    #     "temperature": 0.8,
    #     "repeat_penalty": 1.2,
    #     "presence_penalty": 1.5,
    #     "frequency_penalty": 1.0,
    #     "mirostat": 1,
    #     "mirostat_tau": 0.8,
    #     "mirostat_eta": 0.6,
    #     "penalize_newline": true,
    #     "stop": ["\n", "user:"],
    #     "numa": false,
    #     "num_ctx": 1024,
    #     "num_batch": 2,
    #     "num_gqa": 1,
    #     "num_gpu": 1,
    #     "main_gpu": 0,
    #     "low_vram": false,
    #     "f16_kv": true,
    #     "vocab_only": false,
    #     "use_mmap": true,
    #     "use_mlock": false,
    #     "rope_frequency_base": 1.1,
    #     "rope_frequency_scale": 0.8,
    #     "num_thread": 8
    # }

    return options

def get_num_ctx_by_model_name(model_name: str, default_num_ctx: int = 2048) -> int:
    # num_predict	Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)	int	num_predict 42
    # top_k	Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)	int	top_k 40
    # top_p	Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)

    # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values

    # DEFAULT: 2048
    # mixtral num_ctx:64k    https://ollama.com/library/mixtral
    # llama3 num_ctx: 8k    https://ollama.com/library/llama3-gradient
    # llama3 num_ctx: 128k    https://ollama.com/library/llama3.1
    # llama3-gradient: 256000   <1M https://ollama.com/library/llama3-gradient
    num_ctx: int = default_num_ctx
    match model_name:
        case s if s.startswith("bespoke-minicheck"):
            num_ctx = 32 * 1024
        case s if s.startswith("reader-lm"):
            num_ctx = 256*1024
            # num_ctx = 32 * 1024
        case "hermes3:70b-llama3.1-q4_0":
            num_ctx = 8_192
        case s if s.startswith("hermes3"):
            num_ctx = 64 * 1024  # 128 * 1024 -> 9%CPU
            if s == "hermes3:latest":
                num_ctx = 128 * 1024
        case s if s.find("mixtral") >= 0:
            num_ctx = 64 * 1024
            if s.find("q8_0") >= 0:
                # beim qbit-8 ist 12 das höchste, das "gut" geht...
                num_ctx = 12 * 1024  # 64*1024
        case s if s.find("gradient") >= 0:
            # num_ctx = 64 * 1024
            num_ctx = 32 * 1024
            if s.find("70b") >= 0:
                num_ctx = 12 * 1024
        case s if s.startswith("llama3.1"):
            num_ctx = 64 * 1024
        case s if s.startswith("llama3"):
            num_ctx = 8_192
        case s if s.startswith("gemma"):
            num_ctx = 8_192  #4_096

    return num_ctx



settings: Settings = Settings()


if settings.postgresql.url:
    os.environ["PSQL_DB_URL"] = os.getenv("PSQL_DB_URL", settings.postgresql.url)

os.environ["PSQL_DB_HOST"] = os.getenv("PSQL_DB_HOST", settings.postgresql.host_in_cluster if is_in_cluster() else settings.postgresql.host)
os.environ["PSQL_DB_USERNAME"] = os.getenv("PSQL_DB_USERNAME", settings.postgresql.username)
os.environ["PSQL_DB_PASSWORD"] = os.getenv("PSQL_DB_PASSWORD", settings.postgresql.password)
os.environ["PSQL_DB_NAME"] = os.getenv("PSQL_DB_NAME", settings.postgresql.dbname)
os.environ["PSQL_DB_PORT"] = os.getenv("PSQL_DB_PORT", str(settings.postgresql.port))

# HOST: str = "http://localhost:11434"
# assuming HTTP -> no HTTPS atm
OLLAMA_HOST: str = os.getenv("OLLAMA_BASE_URL", f"http://{settings.ollama.host}:80")

if is_in_cluster():
    OLLAMA_HOST = f"http://{settings.ollama.host_in_cluster}:{settings.ollama.port}"


OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", settings.ollama.ollama_model)
OLLAMA_EMBED_MODEL: str = os.getenv("OLLAMA_MODEL", settings.ollama.ollama_embed_model)

OLLAMA_FUNCTION_CALLING_MODEL: str = os.getenv(
    "OLLAMA_FUNCTION_CALLING_MODEL", settings.ollama.ollama_function_calling_model
)
OLLAMA_GUESS_LANGUAGE_MODEL: str = os.getenv("OLLAMA_GUESS_LANGUAGE_MODEL", settings.ollama.ollama_guess_language_model)
CHROMADB_DEFAULT_COLLECTION_NAME: str = os.getenv("CHROMADB_DEFAULT_COLLECTION_NAME", settings.chromadb.default_collectionname)

ARLEY_AUG_UNIFIED: bool = str2bool(os.getenv("ARLEY_AUG_UNIFIED", settings.arley_aug.unified))
ARLEY_AUG_PER_ITEM: bool = str2bool(os.getenv("ARLEY_AUG_PER_ITEM", settings.arley_aug.per_item))
ARLEY_AUG_NUM_DOCS: int = int(os.getenv("ARLEY_AUG_NUM_DOCS", settings.arley_aug.num_docs))
ARLEY_AUG_TEMPLATE_TYPE: TemplateType = TemplateType[os.getenv("ARLEY_AUG_TEMPLATE_TYPE", settings.arley_aug.template_type.value)]
ARLEY_AUG_ONLY_CONTRACTS: bool = str2bool(os.getenv("ARLEY_AUG_ONLY_CONTRACTS", settings.arley_aug.only_contracts))
ARLEY_AUG_LANG_FILTER: bool = str2bool(os.getenv("ARLEY_AUG_LANG_FILTER", settings.arley_aug.lang_filter))
ARLEY_AUG_FIRST_REQUEST_INCLUDE_AUG: bool = str2bool(os.getenv("ARLEY_AUG_FIRST_REQUEST_INCLUDE_AUG", settings.arley_aug.first_request_include_aug))
ARLEY_AUG_FIRST_REQUEST_UNIFIED: bool = str2bool(os.getenv("ARLEY_AUG_FIRST_REQUEST_UNIFIED", settings.arley_aug.first_request_unified))
ARLEY_AUG_FIRST_REQUEST_PER_ITEM: bool = str2bool(os.getenv("ARLEY_AUG_FIRST_REQUEST_PER_ITEM", settings.arley_aug.first_request_per_item))
ARLEY_AUG_FIRST_REQUEST_TEMPLATE_TYPE: TemplateType = TemplateType[os.getenv("ARLEY_AUG_FIRST_REQUEST_TEMPLATE_TYPE", settings.arley_aug.first_request_template_type.value)]
ARLEY_AUG_FIRST_REQUEST_N_AUG_RESULTS: int = int(os.getenv("ARLEY_AUG_FIRST_REQUEST_N_AUG_RESULTS", settings.arley_aug.first_request_n_aug_results))
ARLEY_AUG_FIRST_REQUEST_AUG_ONLY_CONTRACTS: bool = str2bool(os.getenv("ARLEY_AUG_FIRST_REQUEST_AUG_ONLY_CONTRACTS", settings.arley_aug.first_request_aug_only_contracts))
ARLEY_AUG_FIRST_REQUEST_AUG_LANG_FILTER: bool = str2bool(os.getenv("ARLEY_AUG_FIRST_REQUEST_AUG_LANG_FILTER", settings.arley_aug.first_request_aug_lang_filter))
REFINELOG_RECIPIENTS: list[str]|None = None
if os.getenv("REFINELOG_RECIPIENTS"):
    REFINELOG_RECIPIENTS = os.getenv("REFINELOG_RECIPIENTS").split(",")

# TODO HT 20240917 -> move to proper pydantic-settings submodel-targeting e.g. "ARLEY_AUG_UNIFIED" -> "ARLEY__AUG_UNIFIED"


TEMPLATE_VERSION: str = os.getenv("ARLEY_AUG_TEMPLATE_VERSION", settings.arley_aug.template_version)

TEMPLATEDIRPATH: Path = Path(__file__).parent.resolve()
TEMPLATEDIRPATH = Path(TEMPLATEDIRPATH, "templates")
TEMPLATEDIRPATH = Path(TEMPLATEDIRPATH, TEMPLATE_VERSION)

logger.info(f"Effective templatedirpath: {TEMPLATEDIRPATH}")

if __name__ == "__main__":
    log_settings()

