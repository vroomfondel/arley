# nomic-embed-text:latest
import argparse, sys
import glob
import inspect
from enum import StrEnum, auto
from functools import partial
from io import StringIO

import chromadb
import ruamel.yaml
from llama_index.core.schema import BaseNode, TextNode, MetadataMode
from llama_index.readers.file import FlatReader
from mammoth.results import Result

from arley import Helper
# import yaml

from arley.config import (settings,
                          is_in_cluster,
                          OllamaPrimingMessage,
                          TemplateType,
                          OLLAMA_HOST,
                          get_ollama_options,
                          OLLAMA_GUESS_LANGUAGE_MODEL,
                          OLLAMA_EMBED_MODEL,
                          OLLAMA_MODEL,
                          CHROMADB_DEFAULT_COLLECTION_NAME)

from loguru import logger

from pathlib import Path, PurePath
from typing import Tuple, AnyStr, Literal, Dict, Any, Sequence, Optional, Generator

import openpyxl
from openpyxl.workbook import Workbook
from openpyxl.worksheet.worksheet import Worksheet
import openpyxl.utils.cell

import re

import openparse

from arley.dbobjects.ragdoc import ArleyDocumentInformation, DocTypeEnum

import llama_index
from llama_index.readers.file.markdown import MarkdownReader
from llama_index.core.node_parser import MarkdownNodeParser, LanguageConfig, SemanticDoubleMergingSplitterNodeParser
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core.node_parser import SentenceSplitter

from arley.llm import ollama_adapter
from arley.llm.language_guesser import LanguageGuesser
from arley.llm.ollama_adapter import Message, ask_ollama_chat, get_create_summary_prompt, \
    get_create_outline_prompt, get_reformat_and_semantically_markup_prompt
from arley.vectorstore.chroma_adapter import ChromaDBConnection

_OLLAMA_DEFAULT_MODEL: str = OLLAMA_MODEL  # "nous-hermes2-mixtral:8x7b"  # "hermes3:8b-llama3.1-fp16"  # "hermes3:70b-llama3.1-q4_0"  # "nous-hermes2-mixtral:8x7b"
_OLLAMA_DEFAULT_EMBED_MODEL: str = OLLAMA_EMBED_MODEL  # "nomic-embed-text:latest"  #"mxbai-embed-large:latest"  # "nomic-embed-text:latest"
_CHROMADB_DEFAULT_COLLECTION_NAME: str = CHROMADB_DEFAULT_COLLECTION_NAME

import numpy as np

def docx_to_html(file: Path) -> Path | None:
    # https://github.com/mwilliamson/python-mammoth
    import mammoth

    with open(file, "rb") as docx_file:
        result: Result = mammoth.convert_to_html(docx_file)
        html = result.value # The generated HTML
        messages = result.messages # Any messages, such as warnings during conversion

        outf = Path(file.parent.resolve(), file.name[:file.name.find(".doc")] + ".html")

        # with open(outf, 'w') as html_file:
        outf.write_text(html)
        logger.debug(f"written to: {outf.absolute()}")

        # You can also extract the raw text of the document by using mammoth.extract_raw_text. This will ignore all formatting in the document. Each paragraph is followed by two newlines.
        # with open("document.docx", "rb") as docx_file:
        #     result = mammoth.extract_raw_text(docx_file)
        #     text = result.value # The raw text
        #     messages = result.messages # Any messages

        return outf


def html_to_markdown(file: Path, tags_to_strip: None|list[str] = None) -> Path | None:
    from markdownify import markdownify as md

    # md('<b>Yay</b> <a href="http://github.com">GitHub</a>', convert=['b'])  # > '**Yay** GitHub'

    html_snippet: str = file.read_text()

    markdowned: str = md(html_snippet, strip=tags_to_strip)

    outf = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.html.value)] + MyFileTypes.md.value)
    # with open(outf, 'w') as html_file:
    outf.write_text(markdowned)
    logger.debug(f"written to: {outf.absolute()}")

    return outf


def xlsx_to_yaml_raw(file: Path) -> Path | None:
    outf: Path = Path(file.parent.resolve(), file.name[0:file.name.rfind(".")] + "_raw." + MyFileTypes.yaml.value)
    if outf.exists():
        logger.debug(f"EXISTS: {outf.resolve()}")
        return outf
    outf.touch()

    colname_to_idx: Dict[str, int] = {}
    idx_to_colname: Dict[int, str] = {}

    xlsx_data: Dict[str, list[str, int] | None] = {}

    # OR read with pandas: https://pythonbasics.org/read-excel/

    # Define variable to load the dataframe
    wb: Workbook = openpyxl.load_workbook(file)

    cell: openpyxl.cell.cell.Cell
    # Define variable to read sheet
    sheetnames: list[str] = wb.sheetnames
    for sheetname in sheetnames:
        sheet: Worksheet = wb[sheetname]
        logger.debug(f"{sheetname=} {type(sheet)=} {sheet=}")
        logger.debug(f"{sheet.max_column=} {sheet.max_row=}")

        sheet.delete_rows(idx=0, amount=3)
        # sheetname='NDA_1' type(sheet)=<class 'openpyxl.worksheet.worksheet.Worksheet'> sheet=<Worksheet "NDA_1">
        logger.debug(f"{sheet.max_column=} {sheet.max_row=}")

        for rowidx, row in enumerate(sheet.iter_rows()):  #min_row=1, max_col=3, max_row=2):
            logger.debug(f"{rowidx=} {type(row)=} {row=}")

            for cellidx, cell in enumerate(row):
                # logger.debug(f"{type(cell)=} {cell=}")
                # logger.debug(f"{cell.col_idx=} {cell.coordinate=} {cell.value=}")
                # logger.debug(f"{openpyxl.utils.cell.coordinate_to_tuple(cell.coordinate)=}")

                if rowidx == 0:
                    colname_to_idx[cell.value] = cellidx
                    idx_to_colname[cellidx] = cell.value

                    xlsx_data[cell.value] = []  # type: ignore

                    continue

                if cell.value:
                    value: Any = cell.value
                    if idx_to_colname[cellidx] == "Relevanz":
                        value = value*100
                    elif isinstance(value, str):
                        value = value.strip()

                    xlsx_data[idx_to_colname[cellidx]].append(value)


    for key in xlsx_data:  #.keys():
        values: Any = xlsx_data[key]
        if isinstance(values, list):
            if len(values) == 1:
                xlsx_data[key] = values[0]
            elif len(values) == 0:
                xlsx_data[key] = None

    yaml = ruamel.yaml.YAML()
    yaml.indent(sequence=4, offset=2)

    with open(outf, 'w') as outfile:
        # yaml.dump(xlsx_data, outfile, indent=4, allow_unicode=True, sort_keys=False, default_flow_style=False)
        yaml.dump(xlsx_data, outfile)
        # yaml.dump(xlsx_data, sys.stdout)
        logger.debug(Helper.get_dict_as_yaml_str(xlsx_data))

    return outf


def docx_to_html_via_subprocess(file: Path) -> Tuple[int, Path] | None:
    outf: Path = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.docx.value)] + MyFileTypes.html.value)
    cwd: str = str(file.parent.resolve().absolute())
    cmd_array: list[str] = [
            '/usr/bin/libreoffice',
            '--invisible',
            '--convert-to',
            'html',
            str(file.absolute())
        ]

    ret: int = exec_sub_process(cwd=cwd, cmd_array=cmd_array)

    return ret, outf

def docx_to_md_via_subprocess(file: Path) -> Tuple[int, Path] | None:
    outf_md = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.docx.value)] + MyFileTypes.md.value)
    cwd: str = str(file.parent.resolve().absolute())

    # markdown_mmd markdown markdown_strict gfm commonmark
    cmd_array: list[str] = [
        "/usr/bin/pandoc",
        '-f', 'docx',
        '-t', 'commonmark',
        '-o', str(outf_md.absolute()),
        str(file.absolute())
    ]

    ret: int = exec_sub_process(cwd=cwd, cmd_array=cmd_array)

    return ret, outf_md

def docx_to_txt_via_subprocess(file: Path) -> Tuple[int, Path] | None:
    outf_txt = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.docx.value)] + MyFileTypes.txt.value)
    cwd: str = str(file.parent.resolve().absolute())

    # markdown_mmd markdown markdown_strict gfm commonmark
    cmd_array: list[str] = [
        "/usr/bin/pandoc",
        '-f', 'docx',
        '-t', 'plain',
        '-o', str(outf_txt.absolute()),
        str(file.absolute())
    ]

    ret: int = exec_sub_process(cwd=cwd, cmd_array=cmd_array)

    return ret, outf_txt

def docx_to_pdf_via_subprocess(file: Path) -> Tuple[int, Path] | None:
    outf: Path = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.docx.value)] + MyFileTypes.pdf.value)
    cwd: str = str(file.parent.resolve().absolute())
    cmd_array: list[str] = [
            '/usr/bin/libreoffice',
            '--invisible',
            '--convert-to',
            'pdf',
            str(file.absolute())
        ]

    ret: int = exec_sub_process(cwd=cwd, cmd_array=cmd_array)

    return ret, outf


def html_to_md_via_subprocess(file: Path) -> Tuple[int, Path] | None:
    outf_md = Path(file.parent.resolve(), file.name[:file.name.rfind(MyFileTypes.html.value)] + MyFileTypes.md.value)
    cwd: str = str(file.parent.resolve().absolute())

    cmd_array: list[str] = [
        "/usr/bin/pandoc",
        '-f', 'html',
        '-t', 'markdown_strict',
        '-o', str(outf_md.absolute()),
        str(file.absolute())
    ]

    ret: int = exec_sub_process(cwd=cwd, cmd_array=cmd_array)

    return ret, outf_md

def exec_sub_process(cwd: str, cmd_array: list[str]) -> int | None:
    # BASED ON: https://gist.githubusercontent.com/goerz/8897c2d8a602af2a45d4/raw/d7bf5a5d589389e99c796a555b640f8f4268d93e/doc2markdown.py

    import sys
    import subprocess

    ret: int = subprocess.call(cmd_array,
        stderr=sys.stderr,
        stdout=sys.stdout,
        cwd=cwd
    )

    return ret



# def cosine_similarity(
#     a: np.ndarray | list[float], b: np.ndarray | list[float]
# ) -> float:
#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
#
#
# def get_node_similarities(nodes: list[Node]):
#     # get the similarity of each node with the node that precedes it
#     embeddings = embedding_client.embed_many([node.text for node in nodes])
#     similarities = []
#     for i in range(1, len(embeddings)):
#         similarities.append(cosine_similarity(embeddings[i - 1], embeddings[i]))
#
#     similarities = [round(sim, 2) for sim in similarities]
#     return [0] + similarities



def parse_pdf(file: Path):
    parser: openparse.DocumentParser = openparse.DocumentParser()
    parsed_basic_doc: openparse.schemas.ParsedDocument = parser.parse(file)

    for node in parsed_basic_doc.nodes:
        node_data: dict = node.model_dump(mode="json", by_alias=True, exclude_none=True)
        logger.debug(Helper.get_pretty_dict_json_no_sort(node_data))

    # lala = openparse.processing.SemanticIngestionPipeline()
    # from openparse import processing, DocumentParser
    #
    # semantic_pipeline = processing.SemanticIngestionPipeline(
    #     openai_api_key=OPEN_AI_KEY,
    #     model="text-embedding-3-large",
    #     min_tokens=64,
    #     max_tokens=1024,
    # )
    # parser = DocumentParser(
    #     processing_pipeline=semantic_pipeline,
    # )
    # parsed_content = parser.parse(basic_doc_path)

    return 0

def llm_get_helpfull_atorney_system_prompt(lang: Literal["en", "de"] = "en", return_in_markdown: bool = True) -> str | None:
    priming_msg: OllamaPrimingMessage
    for priming_msg in settings.ollama.ollama_priming_messages:
        if priming_msg.lang != lang or priming_msg.role != "system":
            continue

        ret: str = priming_msg.content
        if return_in_markdown:
            if lang == "en":
                ret = ret.rstrip() + "\nYour output must be formatted in markdown syntax."
            else:
                ret = ret.rstrip() + "\nDeine Ausgabe muß in markdown syntax formatiert sein."
        return ret

    return None


def llm_remove_unnecessary_newlines_and_generate_semantic_markup_file(
        txt_file: Path,
        ollama_model: str = _OLLAMA_DEFAULT_MODEL,
        temperature: float = 0.0,
        template_type: TemplateType=TemplateType.xml) -> Path | None:
    outf: Path = Path(txt_file.parent.resolve(),
                      txt_file.name[:txt_file.name.rfind(".")] + "_llm." + MyFileTypes.md.value)
    outf.touch()

    _txt: str
    with open(txt_file, 'r') as infile:
        _txt = infile.read()

    new_text = llm_remove_unnecessary_newlines_and_generate_semantic_markup(
        plain_txt=_txt,
        ollama_model=ollama_model,
        template_type=template_type,
        temperature=temperature
    )

    with open(outf, 'w') as outfile:
        outfile.write(new_text)
        outfile.flush()

    logger.debug(new_text)

    return outf


# "nous-hermes2-mixtral:8x7b"  # "llama3.1:70b-instruct-q3_K_M"  # "llama3.1:latest"  # "llama3.1:70b-instruct-q4_0"  #"nous-hermes2-mixtral:8x7b"  #"gemma2:27b"  #mixtral:latest"
def llm_remove_unnecessary_newlines_and_generate_semantic_markup(plain_txt: str, ollama_model: str = _OLLAMA_DEFAULT_MODEL, temperature: float = 0.0, template_type: TemplateType=TemplateType.xml) -> str|None:
    ret: str|None

    lang: Literal["en", "de"] = guess_my_language(txt=plain_txt, ollama_model=ollama_model)

    msgs: list[Message] | None = None  # []

    prompt: str = get_reformat_and_semantically_markup_prompt(
        md_or_plaintext=plain_txt,
        lang=lang,
        ollama_model=ollama_model,
        template_type=template_type
    )

    logger.debug(prompt)

    resp: dict = ask_ollama_chat(
        streamed=True,
        system_prompt=None,
        prompt=prompt,
        msg_history=msgs,
        model=ollama_model,
        evict=False,
        temperature=temperature,
        top_k=40,
        top_p=0.9,
        num_predict=-1,
        seed=0,
        print_msgs=False,
        print_response=True,
    )

    new_text: str = resp["message"]["content"]
    ret = new_text

    return ret


def llm_create_outline_file(md_or_plaintxt_file: Path, is_plaintext: bool, ollama_model: str = _OLLAMA_DEFAULT_MODEL, delete_other_llm_base: bool = True, temperature: float = 0.4, template_type: TemplateType=TemplateType.xml) -> Path | None:
    outf_name: str = md_or_plaintxt_file.name[:md_or_plaintxt_file.name.rfind(".")] + "_outline."
    outf_name_other: str = outf_name
    if is_plaintext:
        outf_name += MyFileTypes.txt.value
        outf_name_other += MyFileTypes.md.value
    else:
        outf_name += MyFileTypes.md.value
        outf_name_other += MyFileTypes.txt.value

    outf: Path = Path(md_or_plaintxt_file.parent.resolve(),
                      outf_name)
    outf.touch()

    outf_other: Path = Path(md_or_plaintxt_file.parent.resolve(),
                      outf_name_other)

    if delete_other_llm_base:
        outf_other.unlink(missing_ok=True)

    md_or_plaintxt: str
    with open(md_or_plaintxt_file, 'r') as infile:
        md_or_plaintxt = infile.read()

    outline_text = llm_create_outline(
        md_or_plaintxt=md_or_plaintxt,
        is_plaintext=is_plaintext,
        ollama_model=ollama_model,
        template_type=template_type,
        temperature=temperature
    )

    with open(outf, 'w') as outfile:
        outfile.write(outline_text)
        outfile.flush()

    logger.debug(outline_text)

    return outf

def guess_my_language(txt: str, print_response: bool=True, ollama_model: str = OLLAMA_GUESS_LANGUAGE_MODEL, print_msgs: bool = True, print_detect_txt: bool = True) -> Literal["en", "de"]:
    lang_detect_text: str = f"{txt[:min(len(txt), 512)]}".strip()

    lang: Literal["en", "de"] | None = None
    ollama_response: dict | None = None
    lang_detect_content: dict | None = None

    if print_detect_txt:
        logger.debug(f"\n{lang_detect_text=}\n")

    ret: str | Tuple[str, dict, dict] | None = None
    try:
        ret = LanguageGuesser.guess_language(input_text=lang_detect_text, only_return_str=False,
                                             ollama_host=OLLAMA_HOST, ollama_model=ollama_model,
                                             ollama_options=get_ollama_options(ollama_model), print_msgs=print_msgs,
                                             print_response=print_response, print_http_response=False,
                                             print_http_request=False, max_retries=3)
    except Exception as ex:
        logger.exception(Helper.get_exception_tb_as_string(ex))


    if ret:
        lang, ollama_response, lang_detect_content = ret

    if lang is None:
        lang = "de"
        logger.error("DEFAULTING LANG to de")

    return lang  # type: ignore


# _OLLAMA_DEFAULT_MODEL  # "llama3.1:70b-instruct-q3_K_M"  # "llama3.1:latest"  # "llama3.1:70b-instruct-q4_0"  #_OLLAMA_DEFAULT_MODEL  #"gemma2:27b"  #mixtral:latest"
def llm_create_outline(md_or_plaintxt: str, is_plaintext: bool, ollama_model: str = _OLLAMA_DEFAULT_MODEL, temperature: float = 0.4, template_type: TemplateType=TemplateType.xml) -> str|None:
    ret: str|None

    lang: Literal["en", "de"] = guess_my_language(txt=md_or_plaintxt, ollama_model=ollama_model)

    msgs: list[Message] | None = None  # []

    system_prompt: str = llm_get_helpfull_atorney_system_prompt(lang="en", return_in_markdown=False if is_plaintext else True)  # always en!

    prompt: str = get_create_outline_prompt(
        md_or_plaintext=md_or_plaintxt,
        lang=lang,
        ollama_model=ollama_model,
        template_type=template_type
    )

    logger.debug(system_prompt)
    logger.debug(prompt)

    resp: dict = ask_ollama_chat(
        streamed=True,
        system_prompt=system_prompt,
        prompt=prompt,
        msg_history=msgs,
        model=ollama_model,
        evict=False,
        temperature=temperature,
        top_k=40,
        top_p=0.9,
        num_predict=-1,
        seed=0,
        print_msgs=False,
        print_response=True,
    )

    new_text: str = resp["message"]["content"]
    ret = new_text

    return ret


def llm_create_summary_file(md_or_plaintxt_file: Path, is_plaintext: bool, ollama_model: str = _OLLAMA_DEFAULT_MODEL, delete_other_llm_base: bool = True, temperature: float = 0.4, template_type: TemplateType=TemplateType.xml) -> Path | None:
    outf_name: str = md_or_plaintxt_file.name[:md_or_plaintxt_file.name.rfind(".")] + "_summary."
    outf_name_other: str = outf_name
    if is_plaintext:
        outf_name += MyFileTypes.txt.value
        outf_name_other += MyFileTypes.md.value
    else:
        outf_name += MyFileTypes.md.value
        outf_name_other += MyFileTypes.txt.value

    outf: Path = Path(md_or_plaintxt_file.parent.resolve(),
                      outf_name)
    outf.touch()

    outf_other: Path = Path(md_or_plaintxt_file.parent.resolve(),
                      outf_name_other)

    if delete_other_llm_base:
        outf_other.unlink(missing_ok=True)

    md_or_plaintxt: str
    with open(md_or_plaintxt_file, 'r') as infile:
        md_or_plaintxt = infile.read()

    summary_text = llm_create_summary(
        md_or_plaintxt=md_or_plaintxt,
        is_plaintext=is_plaintext,
        ollama_model=ollama_model,
        temperature=temperature,
        template_type=template_type
    )

    with open(outf, 'w') as outfile:
        outfile.write(summary_text)
        outfile.flush()

    logger.debug(summary_text)

    return outf


# _OLLAMA_DEFAULT_MODEL  # "llama3.1:70b-instruct-q3_K_M"  # "llama3.1:latest"  # "llama3.1:70b-instruct-q4_0"  #_OLLAMA_DEFAULT_MODEL  #"gemma2:27b"  #mixtral:latest"
def llm_create_summary(md_or_plaintxt: str, is_plaintext: bool, ollama_model: str = _OLLAMA_DEFAULT_MODEL, temperature: float = 0.4, template_type: TemplateType=TemplateType.xml) -> str|None:
    ret: str|None = None

    lang: Literal["en", "de"] = guess_my_language(txt=md_or_plaintxt, ollama_model=ollama_model)

    msgs: list[Message] | None = None  # []

    system_prompt: str = llm_get_helpfull_atorney_system_prompt(lang="en", return_in_markdown=False if is_plaintext else True)

    prompt: str = get_create_summary_prompt(
        md_or_plaintext=md_or_plaintxt,
        lang=lang,
        ollama_model=ollama_model,
        template_type=template_type
    )

    logger.debug(system_prompt)
    logger.debug(prompt)

    resp: dict = ask_ollama_chat(
        streamed=True,
        system_prompt=system_prompt,
        prompt=prompt,
        msg_history=msgs,
        model=ollama_model,
        evict=False,
        temperature=temperature,
        top_k=40,
        top_p=0.9,
        num_predict=-1,
        seed=0,
        print_msgs=False,
        print_response=True,
    )

    ret= resp["message"]["content"]
    return ret


def create_excerpt_files(md_or_plaintxt_file: Path, is_plaintext: bool, spacy_mode: bool = False, do_simple_md_split: bool = True, lang: Optional[Literal["en", "de"]] =None, delete_old_excerpts: bool = True) -> list[Path] | None:
    out_dir: Path = Path(md_or_plaintxt_file.parent.resolve(), "llm_excerpts")
    out_dir.mkdir(exist_ok=True)
    existing_files: Generator[Path, None, None] = out_dir.glob("*")
    for f in existing_files:
        if delete_old_excerpts:
            logger.debug(f"Already exists (and will be deleted): ${f.resolve()}")
            f.unlink()
        else:
            logger.debug(f"Already exists (and will be kept): ${f.resolve()}")

    ret: list[Path] = []

    nodes: list[TextNode] = parse_markdown_or_plaintext_semantically(
        md_or_plaintext_file=md_or_plaintxt_file,
        is_plaintext=is_plaintext,
        spacy_mode=spacy_mode,
        do_simple_md_split=do_simple_md_split,
        lang=lang)  # ignore lang for simple split

    for node in nodes:
        logger.debug(
            f"{type(node)=} {node.start_char_idx=} {node.end_char_idx=} {node.get_node_info()=}")

        node_data: dict = node.model_dump(mode="json", by_alias=True, exclude_none=True)
        logger.debug(Helper.get_pretty_dict_json_no_sort(node_data))

        fromchar: int = node.start_char_idx
        tochar: int = node.end_char_idx

        excerpt_name: str = md_or_plaintxt_file.name[:md_or_plaintxt_file.name.rfind(".")] + f"_excerpt_{fromchar}-{tochar}." + MyFileTypes.txt.value

        txt_out: str = node.get_content().strip()

        if len(txt_out) == 0:
            continue

        out_f: Path = Path(out_dir, excerpt_name)
        with open(out_f, 'w') as outfile:
            outfile.write(txt_out)

        ret.append(out_f)


    return ret if len(ret)>0 else None


def parse_markdown_or_plaintext_semantically(md_or_plaintext_file: Path,
                                             is_plaintext: bool,
                                             spacy_mode: bool = False,
                                             do_simple_md_split: bool = True,
                                             lang: Optional[Literal["en", "de"]] = None) -> list[TextNode]:

    md_or_plaintext_docs_flat: list[llama_index.core.schema.Document] = FlatReader().load_data(md_or_plaintext_file)

    if not lang:
        if spacy_mode or not do_simple_md_split:
            lang = guess_my_language(
                txt=md_or_plaintext_docs_flat[0].get_content()
            )
    logger.debug(f"LANG: {lang}")

    doc_nodes: list[TextNode] = semantic_split(is_plaintext=is_plaintext,
                                               spacy_mode=spacy_mode,
                                               lang=lang,
                                               doc=md_or_plaintext_docs_flat[0],
                                               do_simple_md_split=do_simple_md_split
                                               )  # do not pass in already parsed markdown-docs -> no str index and such...
    logger.debug(f"{len(doc_nodes)=}")

    text_out: StringIO = StringIO()
    doc_node: TextNode
    for doc_node in doc_nodes:
        logger.debug(f"{type(doc_node)=} {doc_node.start_char_idx=} {doc_node.end_char_idx=} {doc_node.get_node_info()=}")

        node_data: dict = doc_node.model_dump(mode="json", by_alias=True, exclude_none=True)
        logger.debug(Helper.get_pretty_dict_json_no_sort(node_data))

        text_out.write("\n#####################\n")
        text_out.write(doc_node.get_content().strip())

    logger.debug(text_out.getvalue())

    return doc_nodes

def parse_markdown(file: Path, remove_leading_comment_indent: bool = True, remove_triple_newline: bool = True) -> str | None:
    # md_docs = MarkdownReader().load_data(file)
    md_docs = FlatReader().load_data(file)
    parser: MarkdownNodeParser = MarkdownNodeParser()

    text_out: StringIO = StringIO()

    doc_nodes: list[BaseNode] = parser.get_nodes_from_documents(md_docs, show_progress=True)
    logger.debug(f"{len(doc_nodes)=}")
    for doc_node in doc_nodes:
        # doc_node_parser: MarkdownNodeParser = MarkdownNodeParser()

        text_nodes: list[TextNode] = parser.get_nodes_from_node(doc_node)  # doc_node_parser.get_nodes_from_node(doc_node)
        for node in text_nodes:
            # node_data: dict = node.model_dump(mode="json", by_alias=True, exclude_none=True)
            # logger.debug(Helper.get_pretty_dict_json_no_sort(node_data))
            ct: str = node.get_content()
            if remove_leading_comment_indent:
                ct = re.sub(r'^> ', '', ct, flags=re.MULTILINE)
            if remove_triple_newline:
                ct = re.sub(r'^\n\n\n', '\n', ct, flags=re.MULTILINE)
            text_out.write(ct)

    logger.debug(text_out.getvalue())

    return text_out.getvalue()



def semantic_split(
        is_plaintext: bool, spacy_mode: bool, lang: Optional[Literal["en", "de"]], doc: llama_index.core.schema.Document,
                   ollama_embed_model: str = _OLLAMA_DEFAULT_EMBED_MODEL, do_simple_md_split: bool = True) -> list[TextNode]:
    def my_id_func(prefix, index, document):
        # logger.debug(f"{index=} {document=}")
        return f"{prefix}-my-new-node-id-{index}"

    # sentence_splitter: SentenceSplitter = SentenceSplitter.from_defaults(
    #     include_metadata=True
    # )
    #
    # def my_sentence_splitter_callable(text: str) -> list[str]:
    #     ret: list[str] =  sentence_splitter.split_text(text)
    #     logger.debug(f"{text} -> {len(ret)=}")
    #     return ret

    # logger.debug(f"{type(doc)=} {doc=}")

    semantic_nodes: list[TextNode]

    if spacy_mode:
        raise RuntimeError("spacy mode is not supported at the moment (HT 20240905).")
        # config: LanguageConfig
        # nltk.download("punkt_tab")
        #
        # if lang == "en":
        #     config = LanguageConfig(language="english", spacy_model="en_core_web_md")
        # else:
        #     config = LanguageConfig(language="german", spacy_model="de_core_news_md")
        #
        # dm_semantic_splitter: SemanticDoubleMergingSplitterNodeParser = SemanticDoubleMergingSplitterNodeParser(
        #     language_config=config,
        #     initial_threshold=0.4,
        #     appending_threshold=0.5,
        #     merging_threshold=0.5,
        #     max_chunk_size=5000,
        #     show_progress=True,
        #     merging_range=2
        # )
        #
        # semantic_nodes = dm_semantic_splitter.get_nodes_from_documents([doc])
    else:
        if do_simple_md_split and not is_plaintext:
            parser: MarkdownNodeParser = MarkdownNodeParser()

            semantic_nodes = parser.get_nodes_from_documents([doc], show_progress=True) # type: ignore
            logger.debug(f"{len(semantic_nodes)=}")
            # for doc_node in doc_nodes:
            #     text_nodes: list[TextNode] = parser.get_nodes_from_node(doc_node)  # doc_node_parser.get_nodes_from_node(doc_node)
            #     for node in text_nodes:
            #         node_data: dict = node.model_dump(mode="json", by_alias=True, exclude_none=True)
            #         logger.debug(Helper.get_pretty_dict_json_no_sort(node_data))

        else:
            embed_model: OllamaEmbedding = OllamaEmbedding(
                model_name=ollama_embed_model,
                base_url=ollama_adapter.HOST
            )

            # breakpoint_percentile_threshold=
            # "description": "The percentile of cosine dissimilarity that must be exceeded between a group of sentences and the next to form a node.
            # The smaller this number is, the more nodes will be generated",

            semantic_splitter: SemanticSplitterNodeParser = SemanticSplitterNodeParser.from_defaults(
                include_metadata=True,
                buffer_size=40,
                breakpoint_percentile_threshold=90,
                embed_model=embed_model,
                id_func=partial(my_id_func, "semantic-splitter"),
                # sentence_splitter=my_sentence_splitter_callable
            )

            semantic_nodes: list[BaseNode] = semantic_splitter.get_nodes_from_documents(
                documents=[doc],
                show_progress=True
            )

    return semantic_nodes



    # splitter = SentenceSplitter(
    #     chunk_size=1024,
    #     chunk_overlap=20,
    # )
    # nodes = splitter.get_nodes_from_documents(documents)
    # nodes[0]


    # from llama_index.embeddings.ollama import OllamaEmbedding
    #
    # ollama_embedding = OllamaEmbedding(
    #     model_name="llama2",
    #     base_url="http://localhost:11434",
    #     ollama_additional_kwargs={"mirostat": 0},
    # )
    #
    # pass_embedding = ollama_embedding.get_text_embedding_batch(
    #     ["This is a passage!", "This is another passage"], show_progress=True
    # )
    # logger.debug(pass_embedding)
    #
    # query_embedding = ollama_embedding.get_query_embedding("Where is blue?")
    # logger.debug(query_embedding)
#
#
#
#     from llama_index.core.node_parser import SemanticSplitterNodeParser
#     from llama_index.embeddings.openai import OpenAIEmbedding
#
#     embed_model = OpenAIEmbedding()
#     splitter = SemanticSplitterNodeParser(
#         buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model
#     )
#
#
#     from llama_index.llms.ollama import Ollama
#     llm = Ollama(model="llama3.1:latest", request_timeout=120.0)
#     resp = llm.complete("Who is Paul Graham?")
#     logger.debug(resp)
#
#
#
#     from llama_index.core.llms import ChatMessage
#
#     messages = [
#         ChatMessage(
#             role="system", content="You are a pirate with a colorful personality"
#         ),
#         ChatMessage(role="user", content="What is your name"),
#     ]
#     resp = llm.chat(messages)
#
#
#     from llama_index.core import VectorStoreIndex
#     from llama_index.core import PromptTemplate
#     from llama_index.readers.file import PyMuPDFReader
#     loader = PyMuPDFReader()
#     documents = loader.load(file_path="./data/llama2.pdf")
#
#     from llama_index.core import VectorStoreIndex
#     from llama_index.llms.openai import OpenAI
#
#     gpt35_llm = OpenAI(model="gpt-3.5-turbo")
#     gpt4_llm = OpenAI(model="gpt-4")
#
#     index = VectorStoreIndex.from_documents(documents)
#
#     query_str = "What are the potential risks associated with the use of Llama 2 as mentioned in the context?"
#
#     query_engine = index.as_query_engine(similarity_top_k=2, llm=gpt35_llm)
#
#     # use this for testing
#     vector_retriever = index.as_retriever(similarity_top_k=2)
#     response = query_engine.query(query_str)
#     print(str(response))
#
#     def display_prompt_dict(prompts_dict):
#         for k, p in prompts_dict.items():
#             text_md = f"**Prompt Key**: {k}<br>" f"**Text:** <br>"
#             display(Markdown(text_md))
#             print(p.get_template())
#             display(Markdown("<br><br>"))
#
#     prompts_dict = query_engine.get_prompts()
#     display_prompt_dict(prompts_dict)


class MyFileTypes(StrEnum):
    docx = auto()
    md = auto()
    doc = auto()
    xls = auto()
    xlsx = auto()
    yaml = auto()
    html = auto()
    pdf = auto()
    txt = auto()
    any = auto()



def get_files_by_ext_from_dir(filedir: Path, ext: MyFileTypes, maxret: int = -1) -> None | Path | list[Path]:
    files: list[AnyStr] = glob.glob("*" if ext == ext.any else f"*.{ext.value}", root_dir=filedir)
    if len(files) == 0:
        return None
    elif len(files) == 1 or maxret == 1:
        return Path(filedir, files[0])

    return [Path(filedir, k) for num, k in enumerate(files) if num<maxret or maxret < 0]

def yaml_raw_file_to_yaml_file(yaml_raw_file: Path, file_base_name: str, title: str) -> Path:
    outf: Path = Path(yaml_raw_file.parent.resolve(), file_base_name + "." + MyFileTypes.yaml.value)  # type: ignore
    if outf.exists():
        logger.debug(f"EXISTS: {outf.resolve()}")
        return outf

    adi: ArleyDocumentInformation = ArleyDocumentInformation.from_xls_converted_yaml_file(
        yaml_raw_file,
        title=title,
        doctype=DocTypeEnum.contract.value  # type: ignore
    )


    yaml_data: dict = adi.model_dump(mode="json", by_alias=True, exclude_none=True)

    logger.debug(Helper.get_dict_as_yaml_str(yaml_data))

    with open(outf, 'w') as outfile:
        yaml = ruamel.yaml.YAML()
        yaml.indent(sequence=4, offset=2)

        # yaml.dump(xlsx_data, outfile, indent=4, allow_unicode=True, sort_keys=False, default_flow_style=False)
        yaml.dump(yaml_data, outfile)

    return outf

def main(docdir: Path,
         plaintxt_based: bool = True,
         llm_md_based_if_not_plaintxt_based: bool = True,
         directly_convert_to_txt: bool = True,
         delete_other_llm_base: bool = True) -> int:
    if not docdir.exists() or not docdir.is_dir():
        logger.error(f"{docdir} does not exist or is not a directory")
        return 123

    ret_html: int
    ret_md: int
    ret_pdf: int

    pdf_file: Path
    md_file: Path
    md_llm_file: Path
    html_file: Path
    yaml_file: Path
    yaml_raw_file: Path
    llm_summary_file: Path
    llm_outline_file: Path


    docx_file: Path = get_files_by_ext_from_dir(
        filedir=docdir,
        ext=MyFileTypes.docx,
        maxret=1
    )  # must only be ONE in that dir

    if not docx_file or not docx_file.is_file():
        raise Exception(f"Invalid file {docx_file=}")

    file_basename: str = docx_file.name[0:docx_file.name.rfind(".")]  # type: ignore

    xlsx_file: Path = Path(docx_file.parent, f"{file_basename}.{MyFileTypes.xlsx.value}")
    if not xlsx_file or not xlsx_file.is_file():
        raise Exception(f"Invalid file {xlsx_file=}")

    yaml_raw_file = xlsx_to_yaml_raw(xlsx_file)

    yaml_file = yaml_raw_file_to_yaml_file(
        yaml_raw_file=yaml_raw_file,
        file_base_name=file_basename,
        title=file_basename
    )

    adi: ArleyDocumentInformation = ArleyDocumentInformation.from_yaml_model_file(yaml_file)
    lang: Literal["en", "de"] = adi.lang.value  # type: ignore

    ret_pdf, pdf_file = docx_to_pdf_via_subprocess(docx_file)
    ret_html, html_file = docx_to_html_via_subprocess(docx_file)
    # ret_md, md_file = html_to_md_via_subprocess(html_file)
    ret_md, md_file = docx_to_md_via_subprocess(docx_file)

    txt_file: Path
    if directly_convert_to_txt:
        txt_ret, txt_file = docx_to_txt_via_subprocess(docx_file)
    else:
        txt: str = parse_markdown(file=md_file, remove_leading_comment_indent=True)
        txt_file = Path(docdir, file_basename + "." + MyFileTypes.txt)
        with open(txt_file, 'w') as outfile:
            outfile.write(txt)
            outfile.flush()

    base_file: Path = md_file

    md_llm_file = llm_remove_unnecessary_newlines_and_generate_semantic_markup_file(txt_file)
    if llm_md_based_if_not_plaintxt_based and not plaintxt_based:
        base_file = md_llm_file

    if plaintxt_based:
        base_file = txt_file

    excerpts: list[Path]

    llm_summary_file = llm_create_summary_file(md_or_plaintxt_file=base_file, is_plaintext=plaintxt_based, delete_other_llm_base=delete_other_llm_base)
    llm_outline_file = llm_create_outline_file(md_or_plaintxt_file=base_file, is_plaintext=plaintxt_based, delete_other_llm_base=delete_other_llm_base)

    excerpts = create_excerpt_files(
        md_or_plaintxt_file=base_file,
        is_plaintext=plaintxt_based,
        lang=lang,
        do_simple_md_split=False,
        spacy_mode=False
    )

    for exc in excerpts:
        logger.debug(f"\tExcerpt Written to: {exc.absolute()}")

    logger.debug(f"Written to [{ret_pdf=}]: {pdf_file.absolute()}")
    logger.debug(f"Written to [{ret_html=}]: {html_file.absolute()}")
    logger.debug(f"Written to [{ret_md=}]: {md_file.absolute()}")
    logger.debug(f"Written to: {yaml_raw_file.absolute()}")
    logger.debug(f"Written to: {yaml_file.absolute()}")
    logger.debug(f"Written to: {txt_file.absolute()}")
    logger.debug(f"Written to: {md_llm_file.absolute()}")
    logger.debug(f"Written to: {llm_summary_file.absolute()}")
    logger.debug(f"Written to: {llm_outline_file.absolute()}")

    importtovectorstore(
        adi=adi,
        md_or_plaintxt_file=base_file,
        is_plaintext=plaintxt_based,
        llm_summary_file=llm_summary_file,
        llm_outline_file=llm_outline_file,
        excerpts=excerpts
    )

    return 0

def importdirtovectorstore(
        basedir: Path,
        plaintext_based: bool,
        llm_md_based_if_not_plaintxt_based: bool = True) -> int:
    if not basedir.exists() or not basedir.is_dir():
        raise RuntimeError(f"{basedir.resolve()} does not exist or is not a directory")

    adi: ArleyDocumentInformation | None = None

    txt_base_file: Path | None = None
    md_base_file: Path|None = None
    llm_summary_file: Path|None = None
    llm_outline_file: Path|None = None

    for mefile in basedir.glob("*"):
        logger.debug(f"{mefile.resolve()}")
        if mefile.name.endswith("_llm.md"):
            if llm_md_based_if_not_plaintxt_based:
                md_base_file = mefile
        elif mefile.name.endswith(".md"):
            if not llm_md_based_if_not_plaintxt_based:
                md_base_file = mefile
        elif mefile.name.endswith(".txt"):
            txt_base_file = mefile
        elif mefile.name.endswith("_llm_summary.md") or mefile.name.endswith("_llm_summary.txt"):
            llm_summary_file = mefile
        elif mefile.name.endswith("_llm_outline.md") or mefile.name.endswith("_llm_outline.txt"):
            llm_outline_file = mefile
        elif mefile.name.endswith("_raw.yaml"):
            continue
        elif mefile.name.endswith(".yaml"):
            adi = ArleyDocumentInformation.from_yaml_model_file(mefile)


    base_file: Path = md_base_file
    if plaintext_based:
        base_file = txt_base_file

    excerpts: list[Path] = []
    llm_excerpt_dir: Path = Path(basedir, "llm_excerpts")

    for mefile in llm_excerpt_dir.glob("*.txt"):
        excerpts.append(mefile)

    if not llm_summary_file or not llm_outline_file or not llm_summary_file or not adi or not base_file:
        raise RuntimeError(f"invalid data {adi==None=} {llm_summary_file=} {llm_outline_file=} {base_file=}")

    importtovectorstore(adi=adi,
                        md_or_plaintxt_file=base_file,
                        is_plaintext=plaintext_based,
                        llm_summary_file=llm_summary_file,
                        llm_outline_file=llm_outline_file,
                        excerpts=excerpts
                        )

    return 0


def importtovectorstore(
        adi: ArleyDocumentInformation,
        md_or_plaintxt_file: Path,
        is_plaintext: bool,
        llm_summary_file: Path,
        llm_outline_file: Path,
        excerpts: list[Path],
        collectionname: str = _CHROMADB_DEFAULT_COLLECTION_NAME) -> int:
    cdbconnection: ChromaDBConnection = ChromaDBConnection.get_instance()
    cdbcollection: chromadb.api.models.Collection.Collection = cdbconnection.get_or_create_collection(collectionname)

    fsmap: dict[str, Path] = {
        "file": md_or_plaintxt_file,
        "summary": llm_summary_file,
        "outline": llm_outline_file
    }

    parent_id_for_excerpts: str = str(adi.id)
    parent_file_name_for_excerpts: str = md_or_plaintxt_file.name
    parent_full_path_for_excerpts: str = str(md_or_plaintxt_file.resolve())

    for mefile in excerpts:
        fsmap[str(mefile.resolve())] = mefile

    for filename in fsmap:  #.keys():
        my_id: str = str(adi.id)

        mefile: Path = fsmap[filename]
        md5: str = Helper.get_md5_for_file(mefile)
        metadata: dict = adi.model_dump(mode="json", by_alias=True, exclude_none=True)
        metadata["md5"] = md5

        metadata["file_name"] = mefile.name
        metadata["full_path"] = str(mefile.resolve())

        metadata = Helper.flatten(metadata)
        metadata = Helper.flatten_lists(metadata)

        is_main_file: bool = False

        match filename:
            case "file":
                is_main_file = True
            case "outline":
                metadata["doctype"] = DocTypeEnum.contract_outline.value
                my_id = f"{my_id}_outline"
                metadata["parent_full_path"] = parent_full_path_for_excerpts
                metadata["parent_file_name"] = parent_file_name_for_excerpts
                metadata["parent_id"] = parent_id_for_excerpts
            case "summary":
                metadata["doctype"] = DocTypeEnum.contract_summary.value
                my_id = f"{my_id}_summary"
                metadata["parent_full_path"] = parent_full_path_for_excerpts
                metadata["parent_file_name"] = parent_file_name_for_excerpts
                metadata["parent_id"] = parent_id_for_excerpts
            case _:
                metadata["doctype"] = DocTypeEnum.contract_excerpt.value
                metadata["parent_full_path"] = parent_full_path_for_excerpts
                metadata["parent_file_name"] = parent_file_name_for_excerpts
                metadata["parent_id"] = parent_id_for_excerpts

                range_str: str = mefile.name[mefile.name.rfind("_")+1:]
                range_str: str = range_str[0:range_str.rfind(".")]

                logger.debug(f"{mefile.name=} {range_str=}")

                range_from: int = int(range_str[0:range_str.rfind("-")])
                range_to: int = int(range_str[range_str.rfind("-")+1:])

                my_id = f"{my_id}_excerpt_{range_str}"

        metadata["id"] = my_id

        with open(mefile, 'r') as infile:
            txt: str = infile.read()

            logger.debug(Helper.get_pretty_dict_json_no_sort(metadata))
            logger.debug(txt)

            cdbconnection.add_document(
                cdbcollection=cdbcollection,
                doc_id=my_id,
                document=txt,
                metadata=metadata
            )

        if is_main_file:
            metadata["parent_full_path"] = parent_full_path_for_excerpts
            metadata["parent_file_name"] = parent_file_name_for_excerpts
            metadata["parent_id"] = parent_id_for_excerpts

            metadata["doctype"] = DocTypeEnum.categorization_lookup.value
            my_id = f"{str(adi.id)}_catlookup"

            cdbconnection.add_document(
                cdbcollection=cdbcollection,
                doc_id=my_id,
                document=metadata["categorization_tags"],
                metadata=metadata
            )


            if metadata.get("categorization_targeted_by_prompts"):
                metadata["doctype"] = DocTypeEnum.targeted_by_prompts_lookup.value
                my_id = f"{str(adi.id)}_prompttarget"
                cdbconnection.add_document(
                    cdbcollection=cdbcollection,
                    doc_id=my_id,
                    document=metadata["categorization_targeted_by_prompts"],
                    metadata=metadata
                )

            metadata["doctype"] = DocTypeEnum.title_lookup.value
            my_id = f"{str(adi.id)}_titlelookup"
            cdbconnection.add_document(
                cdbcollection=cdbcollection,
                doc_id=my_id,
                document=metadata["categorization_titles"],
                metadata=metadata
            )


def main_cli() -> int:
    plaintext_based: bool = False
    llm_md_based_if_not_plaintxt_based: bool = True
    directly_convert_to_txt: bool = True

    do_simple_md_split: bool = False
    spacy_mode: bool = False
    delete_old_excerpts: bool = True
    delete_other_llm_base: bool = False


    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        prog='importhelper.py')  # , usage='%(prog)s cmd [options]')

    subparsers = parser.add_subparsers(dest='cmd')
    subparsers.required = True

    #  subparser for convert
    parser_convert = subparsers.add_parser('convert')
    # add a required argument
    parser_convert.add_argument('dir',
                                type=str,
                                nargs="?",
                                default=str(Path(Path.home(),"ki_vertragsmuster"))
                                )

    parser_parsemd = subparsers.add_parser('parsemd')
    # add a required argument
    parser_parsemd.add_argument('file',
                                type=str,
                                nargs="?",
                                default=str(Path(Path.home(),"ki_vertragsmuster/NDA.md"))
                                )

    parser_parsemd = subparsers.add_parser('parsepdf')
    # add a required argument
    parser_parsemd.add_argument('file',
                                type=str,
                                nargs="?",
                                default=str(Path(Path.home(),"ki_vertragsmuster/NDA.pdf"))
                                )

    parser_llmparse = subparsers.add_parser('llmparse')
    # add a required argument
    parser_llmparse.add_argument('file',
                                type=str,
                                nargs="?",
                                default=str(Path(Path.home(),"ki_vertragsmuster/NDA.txt"))
                                )

    parser_llmsummary = subparsers.add_parser('llmsummary')
    # add a required argument
    parser_llmsummary.add_argument('file',
                                 type=str,
                                 nargs="?",
                                 default=str(Path(Path.home(),"ki_vertragsmuster/NDA_llm.md"))
                                 )

    parser_llmoutline = subparsers.add_parser('llmoutline')
    # add a required argument
    parser_llmoutline.add_argument('file',
                                   type=str,
                                   nargs="?",
                                   default=str(Path(Path.home(),"ki_vertragsmuster/NDA_llm.md"))
                                   )

    parser_parsemdsemantically = subparsers.add_parser('parsemdsemantically')
    # add a required argument
    parser_parsemdsemantically.add_argument('file',
                                type=str,
                                nargs="?",
                                default=str(Path(Path.home(),"ki_vertragsmuster/NDA_llm.md"))
                                )

    parser_createexcerptfiles = subparsers.add_parser('createexcerptfiles')
    # add a required argument
    parser_createexcerptfiles.add_argument('file',
                                            type=str,
                                            nargs="?",
                                            default=str(Path(Path.home(),"ki_vertragsmuster/NDA_llm.md"))
                                            )

    parser_import = subparsers.add_parser('importdirtovectorstore')
    # add a required argument
    parser_import.add_argument('dir',
                                           type=str,
                                           nargs="?",
                                           default=str(Path(Path.home(),"ki_vertragsmuster"))
                                           )

    logger.debug(sys.argv)
    ns: argparse.Namespace = parser.parse_args(sys.argv[1:])

    _ret: int = -1
    if ns.cmd == 'convert':
        docdir: Path = Path(ns.dir)
        _ret = main(
            docdir=docdir,
            plaintxt_based=plaintext_based,
            directly_convert_to_txt=directly_convert_to_txt,
            llm_md_based_if_not_plaintxt_based=llm_md_based_if_not_plaintxt_based,
            delete_other_llm_base=delete_other_llm_base
        )
    elif ns.cmd == 'parsepdf':
        pdffile: Path = Path(ns.file)
        _ret = parse_pdf(pdffile)
    elif ns.cmd == 'parsemd':
        mdfile: Path = Path(ns.file)
        _ret = 0 if parse_markdown(mdfile) else 234
    elif ns.cmd == 'parsemdsemantically':
        mdfile: Path = Path(ns.file)
        _ret = 0 if parse_markdown_or_plaintext_semantically(
            md_or_plaintext_file=mdfile,
            is_plaintext=mdfile.name.endswith("txt"),
            lang=None
        ) else 789
    elif ns.cmd == 'createexcerptfiles':
        mdfile: Path = Path(ns.file)
        _ret = 0 if create_excerpt_files(md_or_plaintxt_file=mdfile,
                                         is_plaintext=mdfile.name.endswith("txt"),
                                         do_simple_md_split=do_simple_md_split,
                                         spacy_mode=spacy_mode,
                                         delete_old_excerpts=delete_old_excerpts,
                                         lang=None) else 456
    elif ns.cmd == 'llmparse':
        txtfile: Path = Path(ns.file)
        _ret = 0 if llm_remove_unnecessary_newlines_and_generate_semantic_markup_file(txtfile) else 345
    elif ns.cmd == 'llmsummary':
        mdfile: Path = Path(ns.file)
        _ret = 0 if llm_create_summary_file(mdfile) else 101
    elif ns.cmd == 'llmoutline':
        mdfile: Path = Path(ns.file)
        _ret = 0 if llm_create_outline_file(mdfile) else 910
    elif ns.cmd == 'importdirtovectorstore':
        docdir: Path = Path(ns.dir)
        _ret = 0 if importdirtovectorstore(basedir=docdir,
                                           plaintext_based=plaintext_based,
                                           llm_md_based_if_not_plaintxt_based=llm_md_based_if_not_plaintxt_based
                                           ) else 111

    return _ret

if __name__ == "__main__":
    _OLLAMA_DEFAULT_MODEL = "hermes3:8b-llama3.1-fp16"  # "nous-hermes2-mixtral:8x7b"  # hermes3:70b-llama3.1-q4_0"  # "hermes3:latest"  # hermes3:70b-llama3.1-q4_0"  # "nous-hermes2-mixtral:8x7b"

    exit(main_cli())



    # https://stackoverflow.com/questions/25626109/python-argparse-conditionally-required-arguments/70716254#70716254

    # import argparse
    #
    # # First parse the deciding arguments.
    # deciding_args_parser = argparse.ArgumentParser(add_help=False)
    # deciding_args_parser.add_argument(
    #         '--argument', required=False, action='store_true')
    # deciding_args, _ = deciding_args_parser.parse_known_args()
    #
    # # Create the main parser with the knowledge of the deciding arguments.
    # parser = argparse.ArgumentParser(
    #         description='...', parents=[deciding_args_parser])
    # parser.add_argument('-a', required=deciding_args.argument)
    # parser.add_argument('-b', required=deciding_args.argument)
    # arguments = parser.parse_args()
    #
    # print(arguments)

    # # Add a checker to set the required flags
    # checker = argparse.ArgumentParser()
    # checker.add_argument("--server-1", action="store_true")
    # checker.add_argument("--server-2", action="store_true")
    # checks, _ = checker.parse_known_args()
    #
    # # Create a new main parser, inheriting the argument definitions
    # #   from the checker, to not duplicate definitions.
    # parser = argparse.ArgumentParser(parents=[checker])
    #
    # # Use the flags from `checker` to flip the requirement state
    # #   as needed
    # parser.add_argument("--config-1", required=checks.server_1)
    # parser.add_argument("--config-2", required=checks.server_2)
    #
    # parsed_args = parser.parse_args()

